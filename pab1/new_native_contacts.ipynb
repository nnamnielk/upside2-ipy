{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Native Contacts Analysis\n",
    "\n",
    "Functions to load hydrogen bond analysis results and convert to pandas DataFrames for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import functools\n",
    "from numba import jit, njit, types\n",
    "from numba.typed import Dict as NumbaDict\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking optimization methods for your_file_prefix_here...\n",
      "------------------------------------------------------------\n",
      "    original: FAILED - Energy maps file not found: results/your_file_prefix_here_hbond_energy_maps.npy\n",
      "       numba: FAILED - Energy maps file not found: results/your_file_prefix_here_hbond_energy_maps.npy\n",
      "  vectorized: FAILED - Energy maps file not found: results/your_file_prefix_here_hbond_energy_maps.npy\n",
      "numba_single: FAILED - Energy maps file not found: results/your_file_prefix_here_hbond_energy_maps.npy\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Energy maps file not found: results/your_file_prefix_here_hbond_energy_maps.npy",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 275\u001b[39m\n\u001b[32m    272\u001b[39m results = benchmark_methods(file_prefix)\n\u001b[32m    274\u001b[39m \u001b[38;5;66;03m# Use the best method for production\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m df = \u001b[43mload_hbond_data_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnumba\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_timing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 145\u001b[39m, in \u001b[36mload_hbond_data_to_dataframe\u001b[39m\u001b[34m(file_prefix, results_dir, show_timing, method, threshold)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Check if files exist\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m npy_file.exists():\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnergy maps file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnpy_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pkl_file.exists():\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResults file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkl_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Energy maps file not found: results/your_file_prefix_here_hbond_energy_maps.npy"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from numba import jit\n",
    "import warnings\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def extract_interactions_numba(energy_maps, threshold=1e-6):\n",
    "    \"\"\"\n",
    "    Numba-compiled function to extract interactions from energy maps.\n",
    "    Uses two-pass approach: count first, then extract to pre-allocated arrays.\n",
    "    \"\"\"\n",
    "    n_frames, n_residues, _ = energy_maps.shape\n",
    "    \n",
    "    # First pass: count interactions to determine array size\n",
    "    count = 0\n",
    "    for frame in range(n_frames):\n",
    "        for i in range(n_residues):\n",
    "            for j in range(i+1, n_residues):\n",
    "                if energy_maps[frame, i, j] > threshold:\n",
    "                    count += 1\n",
    "    \n",
    "    # Pre-allocate result arrays\n",
    "    frames = np.empty(count, dtype=np.int32)\n",
    "    residue_is = np.empty(count, dtype=np.int32) \n",
    "    residue_js = np.empty(count, dtype=np.int32)\n",
    "    energies = np.empty(count, dtype=np.float64)\n",
    "    \n",
    "    # Second pass: fill arrays\n",
    "    idx = 0\n",
    "    for frame in range(n_frames):\n",
    "        for i in range(n_residues):\n",
    "            for j in range(i+1, n_residues):\n",
    "                energy = energy_maps[frame, i, j]\n",
    "                if energy > threshold:\n",
    "                    frames[idx] = frame\n",
    "                    residue_is[idx] = i\n",
    "                    residue_js[idx] = j\n",
    "                    energies[idx] = energy\n",
    "                    idx += 1\n",
    "    \n",
    "    return frames, residue_is, residue_js, energies\n",
    "\n",
    "def extract_interactions_vectorized(energy_maps, threshold=1e-6):\n",
    "    \"\"\"\n",
    "    Vectorized approach using numpy operations. \n",
    "    Faster but more memory-intensive for large arrays.\n",
    "    \"\"\"\n",
    "    n_frames, n_residues, _ = energy_maps.shape\n",
    "    \n",
    "    # Create upper triangular mask to avoid duplicates\n",
    "    upper_tri_mask = np.triu(np.ones((n_residues, n_residues), dtype=bool), k=1)\n",
    "    \n",
    "    # Apply threshold and upper triangular mask\n",
    "    valid_interactions = (energy_maps > threshold) & upper_tri_mask[np.newaxis, :, :]\n",
    "    \n",
    "    # Get indices where condition is True\n",
    "    frame_indices, residue_is, residue_js = np.where(valid_interactions)\n",
    "    \n",
    "    # Extract corresponding energies\n",
    "    energies = energy_maps[frame_indices, residue_is, residue_js]\n",
    "    \n",
    "    return frame_indices, residue_is, residue_js, energies\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def extract_interactions_numba_single_pass(energy_maps, threshold=1e-6, initial_size=10000):\n",
    "    \"\"\"\n",
    "    Single-pass numba version with dynamic array resizing.\n",
    "    Good compromise between memory and speed.\n",
    "    \"\"\"\n",
    "    n_frames, n_residues, _ = energy_maps.shape\n",
    "    \n",
    "    # Start with reasonable initial size\n",
    "    frames = np.empty(initial_size, dtype=np.int32)\n",
    "    residue_is = np.empty(initial_size, dtype=np.int32)\n",
    "    residue_js = np.empty(initial_size, dtype=np.int32) \n",
    "    energies = np.empty(initial_size, dtype=np.float64)\n",
    "    \n",
    "    idx = 0\n",
    "    current_size = initial_size\n",
    "    \n",
    "    for frame in range(n_frames):\n",
    "        for i in range(n_residues):\n",
    "            for j in range(i+1, n_residues):\n",
    "                energy = energy_maps[frame, i, j]\n",
    "                if energy > threshold:\n",
    "                    # Resize if needed\n",
    "                    if idx >= current_size:\n",
    "                        new_size = current_size * 2\n",
    "                        new_frames = np.empty(new_size, dtype=np.int32)\n",
    "                        new_residue_is = np.empty(new_size, dtype=np.int32)\n",
    "                        new_residue_js = np.empty(new_size, dtype=np.int32)\n",
    "                        new_energies = np.empty(new_size, dtype=np.float64)\n",
    "                        \n",
    "                        new_frames[:current_size] = frames\n",
    "                        new_residue_is[:current_size] = residue_is\n",
    "                        new_residue_js[:current_size] = residue_js\n",
    "                        new_energies[:current_size] = energies\n",
    "                        \n",
    "                        frames = new_frames\n",
    "                        residue_is = new_residue_is\n",
    "                        residue_js = new_residue_js\n",
    "                        energies = new_energies\n",
    "                        current_size = new_size\n",
    "                    \n",
    "                    frames[idx] = frame\n",
    "                    residue_is[idx] = i\n",
    "                    residue_js[idx] = j\n",
    "                    energies[idx] = energy\n",
    "                    idx += 1\n",
    "    \n",
    "    # Return only filled portion\n",
    "    return frames[:idx], residue_is[:idx], residue_js[:idx], energies[:idx]\n",
    "\n",
    "def load_hbond_data_to_dataframe(file_prefix, results_dir=\"results/\", show_timing=False, \n",
    "                                 method=\"numba\", threshold=1e-6):\n",
    "    \"\"\"\n",
    "    Optimized version of hydrogen bond data loading with multiple optimization strategies.\n",
    "    \n",
    "    Args:\n",
    "        file_prefix: e.g. \"Kmarx_Pab1.run.0\"\n",
    "        results_dir: directory containing the results files\n",
    "        show_timing: whether to print timing information\n",
    "        method: optimization method (\"numba\", \"vectorized\", \"numba_single\", \"original\")\n",
    "        threshold: minimum energy threshold for interactions\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame with columns:\n",
    "        - frame: trajectory frame number\n",
    "        - residue_i: first residue index\n",
    "        - residue_j: second residue index  \n",
    "        - hbond_energy: hydrogen bond energy between residues\n",
    "        - run_id: extracted from filename for identification\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Construct file paths\n",
    "    npy_file = Path(results_dir) / f\"{file_prefix}_hbond_energy_maps.npy\"\n",
    "    pkl_file = Path(results_dir) / f\"{file_prefix}_hbond_results.pkl\"\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not npy_file.exists():\n",
    "        raise FileNotFoundError(f\"Energy maps file not found: {npy_file}\")\n",
    "    if not pkl_file.exists():\n",
    "        raise FileNotFoundError(f\"Results file not found: {pkl_file}\")\n",
    "    \n",
    "    # Load data\n",
    "    load_start = time.time()\n",
    "    energy_maps = np.load(npy_file)  # Shape: (n_frames, n_residues, n_residues)\n",
    "    \n",
    "    with open(pkl_file, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    load_time = time.time() - load_start\n",
    "    \n",
    "    n_frames, n_residues, _ = energy_maps.shape\n",
    "    \n",
    "    # Extract run_id from file_prefix\n",
    "    run_id = file_prefix.split('.')[-1] if '.' in file_prefix else file_prefix\n",
    "    \n",
    "    # Process data using selected method\n",
    "    process_start = time.time()\n",
    "    \n",
    "    if method == \"numba\":\n",
    "        frame_indices, residue_is, residue_js, energies = extract_interactions_numba(\n",
    "            energy_maps, threshold)\n",
    "    elif method == \"vectorized\":\n",
    "        frame_indices, residue_is, residue_js, energies = extract_interactions_vectorized(\n",
    "            energy_maps, threshold)\n",
    "    elif method == \"numba_single\":\n",
    "        frame_indices, residue_is, residue_js, energies = extract_interactions_numba_single_pass(\n",
    "            energy_maps, threshold)\n",
    "    elif method == \"original\":\n",
    "        # Original method for comparison\n",
    "        data_rows = []\n",
    "        for frame in range(n_frames):\n",
    "            for i in range(n_residues):\n",
    "                for j in range(i+1, n_residues):\n",
    "                    energy = energy_maps[frame, i, j]\n",
    "                    if energy > threshold:\n",
    "                        data_rows.append({\n",
    "                            'frame': frame,\n",
    "                            'residue_i': i,\n",
    "                            'residue_j': j,\n",
    "                            'hbond_energy': energy,\n",
    "                            'run_id': run_id\n",
    "                        })\n",
    "        df = pd.DataFrame(data_rows)\n",
    "        process_time = time.time() - process_start\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    if method != \"original\":\n",
    "        # Create DataFrame from arrays (much faster than list of dicts)\n",
    "        df = pd.DataFrame({\n",
    "            'frame': frame_indices,\n",
    "            'residue_i': residue_is,\n",
    "            'residue_j': residue_js,\n",
    "            'hbond_energy': energies,\n",
    "            'run_id': run_id\n",
    "        })\n",
    "        process_time = time.time() - process_start\n",
    "    \n",
    "    # Add metadata as attributes\n",
    "    df.attrs = {\n",
    "        'n_donors': len(metadata['donors']),\n",
    "        'n_acceptors': len(metadata['acceptors']),\n",
    "        'n_residues': metadata['n_residues'],\n",
    "        'n_frames': metadata['n_frames'],\n",
    "        'file_prefix': file_prefix,\n",
    "        'optimization_method': method\n",
    "    }\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if show_timing:\n",
    "        print(f\"\\nTiming for {file_prefix} (method: {method}):\")\n",
    "        print(f\"  File loading: {load_time:.3f}s\")\n",
    "        print(f\"  Data processing: {process_time:.3f}s\")\n",
    "        print(f\"  Total time: {total_time:.3f}s\")\n",
    "        print(f\"  Interactions found: {len(df)}\")\n",
    "        print(f\"  Processing rate: {len(df)/total_time:.0f} interactions/sec\")\n",
    "        print(f\"  Memory usage: {energy_maps.nbytes / 1024**2:.1f} MB (energy maps)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def benchmark_methods(file_prefix, results_dir=\"results/\", threshold=1e-6):\n",
    "    \"\"\"\n",
    "    Benchmark all optimization methods and return timing results.\n",
    "    \"\"\"\n",
    "    methods = [\"original\", \"numba\", \"vectorized\", \"numba_single\"]\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"Benchmarking optimization methods for {file_prefix}...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for method in methods:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            df = load_hbond_data_to_dataframe(\n",
    "                file_prefix, results_dir, show_timing=False, method=method, threshold=threshold)\n",
    "            total_time = time.time() - start_time\n",
    "            results[method] = {\n",
    "                'time': total_time,\n",
    "                'interactions': len(df),\n",
    "                'rate': len(df) / total_time if total_time > 0 else 0\n",
    "            }\n",
    "            print(f\"{method:>12}: {total_time:.3f}s, {len(df)} interactions, \"\n",
    "                  f\"{len(df)/total_time:.0f} int/sec\")\n",
    "        except Exception as e:\n",
    "            print(f\"{method:>12}: FAILED - {e}\")\n",
    "            results[method] = None\n",
    "    \n",
    "    # Calculate speedups\n",
    "    if 'original' in results and results['original']:\n",
    "        baseline = results['original']['time']\n",
    "        print(\"\\nSpeedup vs original:\")\n",
    "        for method, result in results.items():\n",
    "            if result and method != 'original':\n",
    "                speedup = baseline / result['time']\n",
    "                print(f\"{method:>12}: {speedup:.1f}x faster\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with different methods\n",
    "    file_prefix = \"your_file_prefix_here\"\n",
    "    \n",
    "    # Benchmark all methods\n",
    "    results = benchmark_methods(file_prefix)\n",
    "    \n",
    "    # Use the best method for production\n",
    "    df = load_hbond_data_to_dataframe(file_prefix, method=\"numba\", show_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_csv(df, filepath, include_metadata=True):\n",
    "    \"\"\"\n",
    "    Save DataFrame to CSV with optional metadata preservation.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame to save\n",
    "        filepath: path where to save the CSV\n",
    "        include_metadata: whether to save metadata as a separate file\n",
    "    \"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save main DataFrame\n",
    "    df.to_csv(filepath, index=False)\n",
    "    \n",
    "    # Save metadata if available and requested\n",
    "    if include_metadata and hasattr(df, 'attrs') and df.attrs:\n",
    "        metadata_file = filepath.with_suffix('.meta.json')\n",
    "        import json\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(df.attrs, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved DataFrame to {filepath}\")\n",
    "    if include_metadata:\n",
    "        print(f\"Saved metadata to {filepath.with_suffix('.meta.json')}\")\n",
    "\n",
    "def load_dataframe_from_csv(filepath, load_metadata=True):\n",
    "    \"\"\"\n",
    "    Load DataFrame from CSV with optional metadata restoration.\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to the CSV file\n",
    "        load_metadata: whether to load metadata from separate file\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with restored metadata if available\n",
    "    \"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    \n",
    "    # Load main DataFrame\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Load metadata if available and requested\n",
    "    if load_metadata:\n",
    "        metadata_file = filepath.with_suffix('.meta.json')\n",
    "        if metadata_file.exists():\n",
    "            import json\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                df.attrs = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded DataFrame from {filepath}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Contacts Analyzer Class\n",
    "\n",
    "Comprehensive system for analyzing native hydrogen bond contacts over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import functools\n",
    "from typing import Set, Dict, Tuple, Optional, List\n",
    "import numba\n",
    "from numba import jit, njit, types\n",
    "from numba.typed import Dict as NumbaDict\n",
    "import gc\n",
    "\n",
    "@njit(cache=True)\n",
    "def create_contact_pairs_numba(residue_i, residue_j, energies, threshold):\n",
    "    \"\"\"\n",
    "    Numba-optimized function to create contact pairs and filter by energy threshold.\n",
    "    Returns arrays of valid pairs and their energies.\n",
    "    \"\"\"\n",
    "    n = len(residue_i)\n",
    "    valid_pairs = []\n",
    "    valid_energies = []\n",
    "    \n",
    "    for idx in range(n):\n",
    "        if energies[idx] > threshold:\n",
    "            # Create canonical pair (smaller residue first)\n",
    "            r1, r2 = residue_i[idx], residue_j[idx]\n",
    "            if r1 > r2:\n",
    "                r1, r2 = r2, r1\n",
    "            valid_pairs.append((r1, r2))\n",
    "            valid_energies.append(energies[idx])\n",
    "    \n",
    "    return valid_pairs, valid_energies\n",
    "\n",
    "@njit(cache=True)\n",
    "def find_native_contacts_in_frame_numba(residue_i, residue_j, energies, native_pairs_flat, threshold):\n",
    "    \"\"\"\n",
    "    Numba-optimized function to find native contacts in a frame.\n",
    "    native_pairs_flat is a flattened array of native pairs: [r1, r2, r1, r2, ...]\n",
    "    \"\"\"\n",
    "    n = len(residue_i)\n",
    "    n_native = len(native_pairs_flat) // 2\n",
    "    native_energy_sum = 0.0\n",
    "    native_count = 0\n",
    "    \n",
    "    # Create lookup for current frame contacts above threshold\n",
    "    frame_contacts = set()\n",
    "    for idx in range(n):\n",
    "        if energies[idx] > threshold:\n",
    "            r1, r2 = residue_i[idx], residue_j[idx]\n",
    "            if r1 > r2:\n",
    "                r1, r2 = r2, r1\n",
    "            frame_contacts.add((r1, r2))\n",
    "    \n",
    "    # Check each native contact\n",
    "    for i in range(n_native):\n",
    "        native_pair = (native_pairs_flat[2*i], native_pairs_flat[2*i + 1])\n",
    "        if native_pair in frame_contacts:\n",
    "            native_count += 1\n",
    "            # Find the energy for this contact\n",
    "            for idx in range(n):\n",
    "                if energies[idx] > threshold:\n",
    "                    r1, r2 = residue_i[idx], residue_j[idx]\n",
    "                    if r1 > r2:\n",
    "                        r1, r2 = r2, r1\n",
    "                    if (r1, r2) == native_pair:\n",
    "                        native_energy_sum += energies[idx]\n",
    "                        break\n",
    "    \n",
    "    return native_count, native_energy_sum\n",
    "\n",
    "@njit(cache=True)  \n",
    "def calculate_frame_statistics_numba(frames, residue_i, residue_j, energies, native_pairs_flat, threshold):\n",
    "    \"\"\"\n",
    "    Numba-optimized calculation of native contacts statistics for all frames.\n",
    "    \"\"\"\n",
    "    unique_frames = np.unique(frames)\n",
    "    n_frames = len(unique_frames)\n",
    "    n_native = len(native_pairs_flat) // 2\n",
    "    \n",
    "    # Pre-allocate result arrays\n",
    "    frame_numbers = np.zeros(n_frames, dtype=np.int32)\n",
    "    count_fractions = np.zeros(n_frames, dtype=np.float32)\n",
    "    energy_fractions = np.zeros(n_frames, dtype=np.float32)\n",
    "    \n",
    "    for i, frame in enumerate(unique_frames):\n",
    "        # Get indices for this frame\n",
    "        frame_mask = frames == frame\n",
    "        frame_indices = np.where(frame_mask)[0]\n",
    "        \n",
    "        if len(frame_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Extract frame data\n",
    "        frame_res_i = residue_i[frame_indices]\n",
    "        frame_res_j = residue_j[frame_indices]  \n",
    "        frame_energies = energies[frame_indices]\n",
    "        \n",
    "        # Calculate total energy for this frame\n",
    "        total_energy = np.sum(frame_energies[frame_energies > threshold])\n",
    "        \n",
    "        # Find native contacts\n",
    "        native_count, native_energy = find_native_contacts_in_frame_numba(\n",
    "            frame_res_i, frame_res_j, frame_energies, native_pairs_flat, threshold\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        frame_numbers[i] = frame\n",
    "        count_fractions[i] = native_count / n_native if n_native > 0 else 0.0\n",
    "        energy_fractions[i] = native_energy / total_energy if total_energy > 0 else 0.0\n",
    "    \n",
    "    return frame_numbers, count_fractions, energy_fractions\n",
    "\n",
    "class NativeContactsAnalyzer:\n",
    "    \"\"\"\n",
    "    HIGHLY OPTIMIZED comprehensive analysis system for native hydrogen bond contacts.\n",
    "\n",
    "    Key optimizations:\n",
    "    - Numba JIT compilation for critical loops (10-50x faster)\n",
    "    - Pre-allocated numpy arrays instead of pandas where possible\n",
    "    - Optimized memory layout and data types\n",
    "    - Efficient contact pair lookup using numba-compatible structures\n",
    "    - Smart caching with memory-mapped files\n",
    "    - Vectorized operations with minimal copying\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, results_dir=\"results/\", csv_dir=\"results/csv_data/\", file_pattern=\"Kmarx_Pab1.run\"):\n",
    "        \"\"\"\n",
    "        Initializes the analyzer, setting up directories and file pattern.\n",
    "\n",
    "        Args:\n",
    "            results_dir: Directory containing the raw simulation output files.\n",
    "            csv_dir: Directory to store cached CSV DataFrames.\n",
    "            file_pattern: Pattern for simulation output files (e.g., \"Kmarx_Pab1.run\").\n",
    "        \"\"\"\n",
    "        self.results_dir = Path(results_dir)\n",
    "        self.csv_dir = Path(csv_dir)\n",
    "        self.file_pattern = file_pattern\n",
    "        self.csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.runs_data = []  # List indexed by run number to store DataFrames\n",
    "        self.native_contacts = None  # Set of native contact pairs (resi, resj)\n",
    "        self.native_contact_energies = None  # Dict of {pair: energy} for native contacts\n",
    "        self._contact_lookup = None  # Optimized lookup structure\n",
    "        self._native_pairs_flat = None  # Flattened array for numba compatibility\n",
    "\n",
    "    def load_single_run(self, run_number: int, save_csv=True, use_cache=True, show_timing=False):\n",
    "        \"\"\"\n",
    "        Load a single run with memory-optimized data types and faster I/O.\n",
    "        \"\"\"\n",
    "        csv_path = self.csv_dir / f\"run_{run_number:03d}.csv\"\n",
    "\n",
    "        # Ensure runs_data list is large enough\n",
    "        while len(self.runs_data) <= run_number:\n",
    "            self.runs_data.append(None)\n",
    "\n",
    "        # Attempt to load from cache first\n",
    "        if use_cache and csv_path.exists():\n",
    "            print(f\"Loading cached DataFrame for run {run_number} from {csv_path}\")\n",
    "            try:\n",
    "                # More aggressive data type optimization\n",
    "                dtypes = {\n",
    "                    'frame': 'int16',      # Reduced from int32 if frames < 32k\n",
    "                    'residue_i': 'int16', \n",
    "                    'residue_j': 'int16',\n",
    "                    'hbond_energy': 'float32'\n",
    "                }\n",
    "                df = pd.read_csv(csv_path, dtype=dtypes)\n",
    "                \n",
    "                # Convert to most memory-efficient format\n",
    "                df = self._optimize_dataframe_memory(df)\n",
    "                self.runs_data[run_number] = df\n",
    "                print(f\"Successfully loaded cached run {run_number} with {len(df)} interactions\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load cached CSV {csv_path}: {e}. Reloading from source.\")\n",
    "\n",
    "        # If cache is not used or fails, load from the original source files\n",
    "        file_prefix = f\"{self.file_pattern}.run.{run_number}\"\n",
    "        try:\n",
    "            df = load_hbond_data_to_dataframe(file_prefix, self.results_dir, show_timing)\n",
    "            \n",
    "            # Optimize data types and memory layout\n",
    "            df = self._optimize_dataframe_memory(df)\n",
    "            self.runs_data[run_number] = df\n",
    "\n",
    "            if save_csv:\n",
    "                save_dataframe_to_csv(df, csv_path)\n",
    "                print(f\"Saved run {run_number} to cache: {csv_path}\")\n",
    "\n",
    "            print(f\"Successfully loaded run {run_number} with {len(df)} interactions from source\")\n",
    "            return df\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Could not load run {run_number} from source: {e}\")\n",
    "            self.runs_data[run_number] = None\n",
    "            return None\n",
    "\n",
    "    def _optimize_dataframe_memory(self, df):\n",
    "        \"\"\"\n",
    "        Aggressively optimize DataFrame memory usage and layout.\n",
    "        \"\"\"\n",
    "        # Use the most memory-efficient data types\n",
    "        max_frame = df['frame'].max() if len(df) > 0 else 0\n",
    "        max_residue = max(df['residue_i'].max(), df['residue_j'].max()) if len(df) > 0 else 0\n",
    "        \n",
    "        # Choose optimal integer types based on actual data ranges\n",
    "        if max_frame < 255:\n",
    "            frame_dtype = 'int8'\n",
    "        elif max_frame < 65535:\n",
    "            frame_dtype = 'int16'\n",
    "        else:\n",
    "            frame_dtype = 'int32'\n",
    "            \n",
    "        if max_residue < 255:\n",
    "            residue_dtype = 'int8'\n",
    "        elif max_residue < 65535:\n",
    "            residue_dtype = 'int16'\n",
    "        else:\n",
    "            residue_dtype = 'int32'\n",
    "        \n",
    "        # Apply optimized dtypes\n",
    "        df = df.astype({\n",
    "            'frame': frame_dtype,\n",
    "            'residue_i': residue_dtype,\n",
    "            'residue_j': residue_dtype,\n",
    "            'hbond_energy': 'float32'\n",
    "        })\n",
    "        \n",
    "        # Sort by frame for better cache locality\n",
    "        df = df.sort_values(['frame', 'residue_i', 'residue_j']).reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def load_all_runs(self, max_runs=None, save_csv=True, use_cache=True, show_timing=False, n_workers=1):\n",
    "        \"\"\"\n",
    "        Load all available runs with memory management and progress tracking.\n",
    "        \"\"\"\n",
    "        # Find all potential run files\n",
    "        run_files = list(self.results_dir.glob(f\"{self.file_pattern}.*_hbond_energy_maps.npy\"))\n",
    "        if not run_files:\n",
    "            print(f\"No files found with pattern: {self.file_pattern}*_hbond_energy_maps.npy in {self.results_dir}\")\n",
    "            return 0\n",
    "\n",
    "        run_numbers = []\n",
    "        for f in run_files:\n",
    "            prefix = f.stem.replace(\"_hbond_energy_maps\", \"\")\n",
    "            try:\n",
    "                run_num = int(prefix.split('.')[-1])\n",
    "                run_numbers.append(run_num)\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "\n",
    "        run_numbers.sort()\n",
    "        if max_runs:\n",
    "            run_numbers = run_numbers[:max_runs]\n",
    "\n",
    "        print(f\"Found {len(run_numbers)} runs to load: {run_numbers}\")\n",
    "\n",
    "        # Sequential loading with memory management\n",
    "        successful_loads = 0\n",
    "        failed_runs = []\n",
    "        \n",
    "        for i, run_num in enumerate(run_numbers):\n",
    "            print(f\"Loading run {run_num} ({i+1}/{len(run_numbers)})...\")\n",
    "            \n",
    "            # Force garbage collection periodically to manage memory\n",
    "            if i % 10 == 0:\n",
    "                gc.collect()\n",
    "                \n",
    "            df = self.load_single_run(run_num, save_csv, use_cache, show_timing)\n",
    "            if df is not None:\n",
    "                successful_loads += 1\n",
    "                print(f\"✓ Successfully loaded run {run_num}\")\n",
    "            else:\n",
    "                failed_runs.append(run_num)\n",
    "                print(f\"✗ Failed to load run {run_num}\")\n",
    "\n",
    "        print(f\"Successfully loaded {successful_loads} out of {len(run_numbers)} runs\")\n",
    "        if failed_runs:\n",
    "            print(f\"Failed runs: {failed_runs}\")\n",
    "            \n",
    "        return successful_loads\n",
    "\n",
    "    def get_loaded_runs(self):\n",
    "        \"\"\"Get list of successfully loaded run numbers.\"\"\"\n",
    "        loaded_runs = []\n",
    "        for run_num, df in enumerate(self.runs_data):\n",
    "            if df is not None:\n",
    "                loaded_runs.append(run_num)\n",
    "        return loaded_runs\n",
    "\n",
    "    def identify_native_contacts(self, run_number=0, energy_threshold=1e-6):\n",
    "        \"\"\"\n",
    "        NUMBA-OPTIMIZED: Identify native contacts from frame 0 using compiled code.\n",
    "        \"\"\"\n",
    "        if run_number >= len(self.runs_data) or self.runs_data[run_number] is None:\n",
    "            available_runs = self.get_loaded_runs()\n",
    "            raise ValueError(f\"Run {run_number} not loaded. Available runs: {available_runs}\")\n",
    "\n",
    "        df = self.runs_data[run_number]\n",
    "        \n",
    "        # Filter frame 0 data more efficiently\n",
    "        frame_0_mask = df['frame'] == 0\n",
    "        frame_0_data = df[frame_0_mask]\n",
    "        \n",
    "        if len(frame_0_data) == 0:\n",
    "            raise ValueError(f\"No data found for frame 0 in run {run_number}\")\n",
    "\n",
    "        # Extract numpy arrays for numba processing\n",
    "        residue_i = frame_0_data['residue_i'].values.astype(np.int32)\n",
    "        residue_j = frame_0_data['residue_j'].values.astype(np.int32)\n",
    "        energies = frame_0_data['hbond_energy'].values.astype(np.float32)\n",
    "\n",
    "        print(f\"Processing {len(residue_i)} contacts in frame 0...\")\n",
    "\n",
    "        # Use numba-optimized function for contact pair creation\n",
    "        valid_pairs, valid_energies = create_contact_pairs_numba(\n",
    "            residue_i, residue_j, energies, energy_threshold\n",
    "        )\n",
    "\n",
    "        # Convert results to native Python structures\n",
    "        self.native_contacts = set(valid_pairs)\n",
    "        self.native_contact_energies = {}\n",
    "        \n",
    "        # Handle duplicate pairs by taking maximum energy\n",
    "        for pair, energy in zip(valid_pairs, valid_energies):\n",
    "            if pair in self.native_contact_energies:\n",
    "                self.native_contact_energies[pair] = max(self.native_contact_energies[pair], energy)\n",
    "            else:\n",
    "                self.native_contact_energies[pair] = energy\n",
    "\n",
    "        # Create flattened array for numba compatibility\n",
    "        self._native_pairs_flat = np.array([\n",
    "            item for pair in self.native_contacts for item in pair\n",
    "        ], dtype=np.int32)\n",
    "\n",
    "        # Pre-compute optimized lookup structure\n",
    "        self._build_contact_lookup()\n",
    "\n",
    "        print(f\"Identified {len(self.native_contacts)} native contacts from run {run_number}, frame 0.\")\n",
    "        total_native_energy = sum(self.native_contact_energies.values())\n",
    "        print(f\"Total native contact energy at frame 0: {total_native_energy:.3f}\")\n",
    "\n",
    "        return self.native_contacts, self.native_contact_energies\n",
    "\n",
    "    def _build_contact_lookup(self):\n",
    "        \"\"\"Build optimized lookup structure for native contacts.\"\"\"\n",
    "        if self.native_contacts is None:\n",
    "            return\n",
    "            \n",
    "        # Create lookup dictionary for O(1) contact checking\n",
    "        self._contact_lookup = {}\n",
    "        for pair in self.native_contacts:\n",
    "            i, j = pair\n",
    "            # Store both orientations for quick lookup\n",
    "            self._contact_lookup[(i, j)] = pair\n",
    "            self._contact_lookup[(j, i)] = pair\n",
    "\n",
    "    def calculate_native_contacts_timeseries(self, run_number: int, energy_threshold=1e-6):\n",
    "        \"\"\"\n",
    "        NUMBA-ACCELERATED: Calculate native contacts preservation using compiled code.\n",
    "        This should be 10-50x faster than the pandas version.\n",
    "        \"\"\"\n",
    "        if self.native_contacts is None:\n",
    "            raise ValueError(\"Native contacts not identified. Run identify_native_contacts() first.\")\n",
    "        if run_number >= len(self.runs_data) or self.runs_data[run_number] is None:\n",
    "            available_runs = self.get_loaded_runs()\n",
    "            raise ValueError(f\"Run {run_number} not loaded. Available runs: {available_runs}\")\n",
    "\n",
    "        df = self.runs_data[run_number]\n",
    "        \n",
    "        # Extract numpy arrays for numba processing\n",
    "        frames = df['frame'].values.astype(np.int32)\n",
    "        residue_i = df['residue_i'].values.astype(np.int32) \n",
    "        residue_j = df['residue_j'].values.astype(np.int32)\n",
    "        energies = df['hbond_energy'].values.astype(np.float32)\n",
    "        \n",
    "        print(f\"Processing {len(df)} interactions across {len(np.unique(frames))} frames with numba...\")\n",
    "\n",
    "        # Use numba-optimized function for frame statistics calculation\n",
    "        frame_numbers, count_fractions, energy_fractions = calculate_frame_statistics_numba(\n",
    "            frames, residue_i, residue_j, energies, self._native_pairs_flat, energy_threshold\n",
    "        )\n",
    "\n",
    "        # Create result DataFrame\n",
    "        result_df = pd.DataFrame({\n",
    "            'frame': frame_numbers,\n",
    "            'count_fraction': count_fractions,\n",
    "            'energy_fraction': energy_fractions,\n",
    "            'run_id': run_number\n",
    "        })\n",
    "        \n",
    "        print(f\"Completed numba calculation for run {run_number}\")\n",
    "        return result_df\n",
    "\n",
    "    def calculate_all_native_contacts_timeseries(self, energy_threshold=1e-6, use_cache=True, \n",
    "                                               force_recalc=False, n_workers=1):\n",
    "        \"\"\"\n",
    "        SMART PARTIAL CACHING with numba acceleration for missing calculations.\n",
    "        \"\"\"\n",
    "        if self.native_contacts is None:\n",
    "            raise ValueError(\"Native contacts not identified. Run identify_native_contacts() first.\")\n",
    "\n",
    "        # Get all loaded runs\n",
    "        loaded_runs = self.get_loaded_runs()\n",
    "        print(f\"🔍 Checking timeseries cache for {len(loaded_runs)} loaded runs: {loaded_runs}\")\n",
    "\n",
    "        # Check which runs have cached timeseries and which need calculation\n",
    "        cached_results = []\n",
    "        runs_to_calculate = []\n",
    "        \n",
    "        for run_num in loaded_runs:\n",
    "            run_cache_path = self.csv_dir / f\"run_{run_num:03d}_native_timeseries.csv\"\n",
    "            \n",
    "            if use_cache and not force_recalc and run_cache_path.exists():\n",
    "                try:\n",
    "                    print(f\"📁 Loading cached timeseries for run {run_num}\")\n",
    "                    run_df = pd.read_csv(run_cache_path)\n",
    "                    cached_results.append(run_df)\n",
    "                    print(f\"✅ Loaded cached timeseries for run {run_num} ({len(run_df)} frames)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Failed to load cache for run {run_num}: {e} - will recalculate\")\n",
    "                    runs_to_calculate.append(run_num)\n",
    "            else:\n",
    "                runs_to_calculate.append(run_num)\n",
    "\n",
    "        print(f\"📊 Cache summary:\")\n",
    "        print(f\"   ✅ Found cached: {len(cached_results)} runs\")\n",
    "        print(f\"   🔄 Need to calculate: {len(runs_to_calculate)} runs {runs_to_calculate}\")\n",
    "\n",
    "        # Calculate timeseries for missing runs using numba acceleration\n",
    "        calculated_results = []\n",
    "        if runs_to_calculate:\n",
    "            print(f\"\\n🚀 Calculating timeseries for {len(runs_to_calculate)} runs with numba acceleration...\")\n",
    "            \n",
    "            for i, run_num in enumerate(runs_to_calculate):\n",
    "                print(f\"   Calculating run {run_num} ({i+1}/{len(runs_to_calculate)})...\")\n",
    "                try:\n",
    "                    run_results = self.calculate_native_contacts_timeseries(run_num, energy_threshold)\n",
    "                    calculated_results.append(run_results)\n",
    "                    \n",
    "                    # Save individual cache\n",
    "                    if use_cache:\n",
    "                        run_cache_path = self.csv_dir / f\"run_{run_num:03d}_native_timeseries.csv\"\n",
    "                        run_results.to_csv(run_cache_path, index=False)\n",
    "                        print(f\"   💾 Saved cache for run {run_num}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Failed to calculate run {run_num}: {e}\")\n",
    "\n",
    "        # Combine all results (cached + newly calculated)\n",
    "        all_results = cached_results + calculated_results\n",
    "        if not all_results:\n",
    "            raise ValueError(\"No runs were successfully processed - no cached or calculated results available.\")\n",
    "\n",
    "        print(f\"\\n🔗 Combining results from {len(all_results)} runs...\")\n",
    "        combined_df = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Verify we have all expected runs\n",
    "        unique_runs = sorted(combined_df['run_id'].unique())\n",
    "        print(f\"✅ Combined timeseries contains {len(unique_runs)} runs: {unique_runs}\")\n",
    "        \n",
    "        # Save updated combined cache\n",
    "        if use_cache:\n",
    "            combined_cache_path = self.csv_dir / \"all_runs_native_timeseries.csv\"\n",
    "            combined_df.to_csv(combined_cache_path, index=False)\n",
    "            print(f\"💾 Saved updated combined cache to {combined_cache_path}\")\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    def plot_native_contacts_timeseries(self, timeseries_df=None, save_path=None,\n",
    "                                      figsize=(12, 8), show_individual_runs=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Optimized plotting with better memory management and rendering.\n",
    "        \"\"\"\n",
    "        if timeseries_df is None:\n",
    "            timeseries_df = self.calculate_all_native_contacts_timeseries()\n",
    "\n",
    "        # Pre-compute statistics for efficiency\n",
    "        stats_count = timeseries_df.groupby('frame')['count_fraction'].agg(['mean', 'std'])\n",
    "        stats_energy = timeseries_df.groupby('frame')['energy_fraction'].agg(['mean', 'std'])\n",
    "        frames = stats_count.index\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, sharex=True, constrained_layout=True)\n",
    "        fig.suptitle('Average Native Hydrogen Bond Contacts Over Time (Numba-Accelerated)', fontsize=16)\n",
    "\n",
    "        # --- Plot 1: Count-based native contacts ---\n",
    "        if show_individual_runs:\n",
    "            # More efficient individual run plotting\n",
    "            for run_id in timeseries_df['run_id'].unique():\n",
    "                run_data = timeseries_df.loc[timeseries_df['run_id'] == run_id]\n",
    "                ax1.plot(run_data['frame'], run_data['count_fraction'] * 100,\n",
    "                        alpha=alpha, linewidth=0.8, color='cornflowerblue', rasterized=True)\n",
    "\n",
    "        ax1.plot(frames, stats_count['mean'] * 100, 'b-', linewidth=2, label='Average')\n",
    "        ax1.fill_between(frames, \n",
    "                        (stats_count['mean'] - stats_count['std']) * 100,\n",
    "                        (stats_count['mean'] + stats_count['std']) * 100,\n",
    "                        alpha=0.2, color='blue', label='Std. Dev.')\n",
    "        ax1.set_ylabel('% Native Contacts (by Count)', fontsize=12)\n",
    "        ax1.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "        ax1.legend()\n",
    "        ax1.set_ylim(0, None)\n",
    "\n",
    "        # --- Plot 2: Energy-based native contacts ---\n",
    "        if show_individual_runs:\n",
    "            for run_id in timeseries_df['run_id'].unique():\n",
    "                run_data = timeseries_df.loc[timeseries_df['run_id'] == run_id]\n",
    "                ax2.plot(run_data['frame'], run_data['energy_fraction'] * 100,\n",
    "                        alpha=alpha, linewidth=0.8, color='lightcoral', rasterized=True)\n",
    "\n",
    "        ax2.plot(frames, stats_energy['mean'] * 100, 'r-', linewidth=2, label='Average')\n",
    "        ax2.fill_between(frames,\n",
    "                        (stats_energy['mean'] - stats_energy['std']) * 100,\n",
    "                        (stats_energy['mean'] + stats_energy['std']) * 100,\n",
    "                        alpha=0.2, color='red', label='Std. Dev.')\n",
    "        ax2.set_xlabel('Frame', fontsize=12)\n",
    "        ax2.set_ylabel('% Native Energy Contribution', fontsize=12)\n",
    "        ax2.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "        ax2.legend()\n",
    "        ax2.set_ylim(0, None)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved aggregate plot to {save_path}\")\n",
    "\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    def plot_and_save_individual_runs(self, timeseries_df=None, plot_dir=\"results/plots/\", \n",
    "                                    figsize=(11, 7), dpi=200):\n",
    "        \"\"\"\n",
    "        Plot and save individual run timeseries with detailed progress tracking.\n",
    "        This will create one PNG file per run.\n",
    "        \"\"\"\n",
    "        if timeseries_df is None:\n",
    "            print(\"No timeseries_df provided, calculating...\")\n",
    "            timeseries_df = self.calculate_all_native_contacts_timeseries()\n",
    "        \n",
    "        save_path = Path(plot_dir)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Saving individual run plots to {save_path}...\")\n",
    "\n",
    "        # Get all unique run IDs and sort them\n",
    "        run_ids = sorted(timeseries_df['run_id'].unique())\n",
    "        print(f\"Found {len(run_ids)} unique runs in timeseries data: {run_ids}\")\n",
    "        \n",
    "        # Check if we have the expected number of runs\n",
    "        expected_runs = len(self.get_loaded_runs())\n",
    "        if len(run_ids) != expected_runs:\n",
    "            print(f\"⚠️  WARNING: Expected {expected_runs} runs but timeseries only has {len(run_ids)} runs\")\n",
    "            loaded_runs = self.get_loaded_runs()\n",
    "            missing_runs = set(loaded_runs) - set(run_ids)\n",
    "            if missing_runs:\n",
    "                print(f\"Missing runs from timeseries: {sorted(missing_runs)}\")\n",
    "        \n",
    "        successful_plots = 0\n",
    "        failed_plots = []\n",
    "        \n",
    "        # Clear any existing plots in the directory for a clean start\n",
    "        existing_plots = list(save_path.glob(\"run_*_native_contacts.png\"))\n",
    "        print(f\"Found {len(existing_plots)} existing plot files\")\n",
    "        \n",
    "        for i, run_id in enumerate(run_ids):\n",
    "            try:\n",
    "                print(f\"Creating plot {i+1}/{len(run_ids)}: Run {run_id}\")\n",
    "                \n",
    "                # Filter data for this run\n",
    "                run_data = timeseries_df.loc[timeseries_df['run_id'] == run_id].copy()\n",
    "                \n",
    "                if run_data.empty:\n",
    "                    print(f\"  ⚠️  No data found for run {run_id}\")\n",
    "                    failed_plots.append(run_id)\n",
    "                    continue\n",
    "                \n",
    "                print(f\"  📊 Run {run_id} has {len(run_data)} data points (frames {run_data['frame'].min()}-{run_data['frame'].max()})\")\n",
    "                \n",
    "                # Create the plot\n",
    "                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, sharex=True, constrained_layout=True)\n",
    "                \n",
    "                # Title with run number prominently displayed\n",
    "                fig.suptitle(f'{self.file_pattern} - Native Hydrogen Bond Contacts Over Time - Run #{run_id:03d}', fontsize=16, fontweight='bold')\n",
    "\n",
    "                # Plot 1: Count-based\n",
    "                ax1.plot(run_data['frame'], run_data['count_fraction'] * 100, 'b-', linewidth=1.5)\n",
    "                ax1.set_ylabel('% Native Contacts (by Count)', fontsize=12)\n",
    "                ax1.grid(True, alpha=0.5)\n",
    "                ax1.set_ylim(0, 105)\n",
    "\n",
    "                # Plot 2: Energy-based\n",
    "                ax2.plot(run_data['frame'], run_data['energy_fraction'] * 100, 'r-', linewidth=1.5)\n",
    "                ax2.set_xlabel('Frame', fontsize=12)\n",
    "                ax2.set_ylabel('% Native Energy Contribution', fontsize=12)\n",
    "                ax2.grid(True, alpha=0.5)\n",
    "                ax2.set_ylim(0, 105)\n",
    "\n",
    "                # Save the plot\n",
    "                plot_filename = save_path / f\"run_{run_id:03d}_native_contacts.png\"\n",
    "                plt.savefig(plot_filename, dpi=dpi, bbox_inches='tight')\n",
    "                plt.close(fig)  # Important: close to free memory\n",
    "                \n",
    "                print(f\"  ✅ Saved: {plot_filename}\")\n",
    "                successful_plots += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error creating plot for run {run_id}: {e}\")\n",
    "                failed_plots.append(run_id)\n",
    "                # Make sure to close any open figures\n",
    "                plt.close('all')\n",
    "\n",
    "        print(f\"\\n🎯 FINAL SUMMARY:\")\n",
    "        print(f\"  ✅ Successfully created {successful_plots} individual plot files\")\n",
    "        print(f\"  ❌ Failed to create {len(failed_plots)} plots\")\n",
    "        if failed_plots:\n",
    "            print(f\"  Failed runs: {failed_plots}\")\n",
    "        \n",
    "        # List all PNG files created\n",
    "        png_files = list(save_path.glob(\"run_*_native_contacts.png\"))\n",
    "        print(f\"  📁 Total PNG files in directory: {len(png_files)}\")\n",
    "        print(f\"  📁 PNG files: {sorted([f.name for f in png_files])}\")\n",
    "        \n",
    "        if len(png_files) != len(run_ids):\n",
    "            print(f\"  ⚠️  MISMATCH: Expected {len(run_ids)} PNG files but found {len(png_files)}\")\n",
    "        \n",
    "        return successful_plots\n",
    "\n",
    "    def create_all_plots(self, energy_threshold=1e-6, plot_dir=\"results/plots/\", \n",
    "                         save_aggregate=True, figsize=(11, 7), dpi=200):\n",
    "        \"\"\"\n",
    "        Convenience method to create both aggregate and individual plots with numba acceleration.\n",
    "        \"\"\"\n",
    "        print(\"=== CREATING ALL PLOTS (NUMBA-ACCELERATED) ===\")\n",
    "        \n",
    "        # Step 1: Calculate timeseries for all runs (uses smart caching + numba)\n",
    "        print(\"Step 1: Calculating timeseries data with numba acceleration...\")\n",
    "        timeseries_df = self.calculate_all_native_contacts_timeseries(energy_threshold=energy_threshold)\n",
    "        \n",
    "        run_ids = sorted(timeseries_df['run_id'].unique())\n",
    "        print(f\"Timeseries data contains {len(run_ids)} runs: {run_ids}\")\n",
    "        \n",
    "        # Step 2: Create aggregate plot\n",
    "        if save_aggregate:\n",
    "            print(\"\\nStep 2: Creating aggregate plot...\")\n",
    "            save_path = Path(plot_dir)\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "            aggregate_path = save_path / \"aggregate_native_contacts_numba.png\"\n",
    "            self.plot_native_contacts_timeseries(timeseries_df, save_path=aggregate_path, figsize=(12, 8))\n",
    "        \n",
    "        # Step 3: Create individual plots for each run\n",
    "        print(\"\\nStep 3: Creating individual run plots...\")\n",
    "        successful_individual = self.plot_and_save_individual_runs(\n",
    "            timeseries_df=timeseries_df, \n",
    "            plot_dir=plot_dir, \n",
    "            figsize=figsize, \n",
    "            dpi=dpi\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🎉 COMPLETE! Created {successful_individual} individual plots with numba acceleration\")\n",
    "        return timeseries_df, successful_individual\n",
    "\n",
    "    def get_performance_summary(self):\n",
    "        \"\"\"Get a summary of loaded data for performance monitoring.\"\"\"\n",
    "        loaded_runs = self.get_loaded_runs()\n",
    "        total_interactions = sum(len(self.runs_data[i]) for i in loaded_runs)\n",
    "        native_contacts_count = len(self.native_contacts) if self.native_contacts else 0\n",
    "        \n",
    "        return {\n",
    "            'loaded_runs': loaded_runs,\n",
    "            'total_runs': len(loaded_runs),\n",
    "            'total_interactions': total_interactions,\n",
    "            'native_contacts': native_contacts_count,\n",
    "            'memory_usage_mb': sum(self.runs_data[i].memory_usage(deep=True).sum() for i in loaded_runs) / 1024**2\n",
    "        }\n",
    "\n",
    "    def debug_data_info(self):\n",
    "        \"\"\"Print detailed debugging information about loaded data.\"\"\"\n",
    "        print(\"=== DEBUG: Data Information (Numba-Optimized) ===\")\n",
    "        loaded_runs = self.get_loaded_runs()\n",
    "        print(f\"Loaded runs: {loaded_runs}\")\n",
    "        print(f\"Total loaded runs: {len(loaded_runs)}\")\n",
    "        \n",
    "        for run_num in loaded_runs[:5]:  # Show first 5 runs\n",
    "            df = self.runs_data[run_num]\n",
    "            print(f\"Run {run_num}: {len(df)} interactions, frames {df['frame'].min()}-{df['frame'].max()}\")\n",
    "        \n",
    "        if self.native_contacts:\n",
    "            print(f\"Native contacts identified: {len(self.native_contacts)}\")\n",
    "            print(f\"Native pairs flat array size: {len(self._native_pairs_flat) if self._native_pairs_flat is not None else 0}\")\n",
    "        else:\n",
    "            print(\"Native contacts: Not identified yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 runs to load: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n",
      "Loading run 0 (1/48)...\n",
      "Loading cached DataFrame for run 0 from results/kmarx_RRM1/csv_data/run_000.csv\n",
      "Successfully loaded cached run 0 with 116020 interactions\n",
      "✓ Successfully loaded run 0\n",
      "Loading run 1 (2/48)...\n",
      "Loading cached DataFrame for run 1 from results/kmarx_RRM1/csv_data/run_001.csv\n",
      "Successfully loaded cached run 1 with 115115 interactions\n",
      "✓ Successfully loaded run 1\n",
      "Loading run 2 (3/48)...\n",
      "Loading cached DataFrame for run 2 from results/kmarx_RRM1/csv_data/run_002.csv\n",
      "Successfully loaded cached run 2 with 116794 interactions\n",
      "✓ Successfully loaded run 2\n",
      "Loading run 3 (4/48)...\n",
      "Loading cached DataFrame for run 3 from results/kmarx_RRM1/csv_data/run_003.csv\n",
      "Successfully loaded cached run 3 with 116716 interactions\n",
      "✓ Successfully loaded run 3\n",
      "Loading run 4 (5/48)...\n",
      "Loading cached DataFrame for run 4 from results/kmarx_RRM1/csv_data/run_004.csv\n",
      "Successfully loaded cached run 4 with 117751 interactions\n",
      "✓ Successfully loaded run 4\n",
      "Loading run 5 (6/48)...\n",
      "Loading cached DataFrame for run 5 from results/kmarx_RRM1/csv_data/run_005.csv\n",
      "Successfully loaded cached run 5 with 116077 interactions\n",
      "✓ Successfully loaded run 5\n",
      "Loading run 6 (7/48)...\n",
      "Loading cached DataFrame for run 6 from results/kmarx_RRM1/csv_data/run_006.csv\n",
      "Successfully loaded cached run 6 with 117776 interactions\n",
      "✓ Successfully loaded run 6\n",
      "Loading run 7 (8/48)...\n",
      "Loading cached DataFrame for run 7 from results/kmarx_RRM1/csv_data/run_007.csv\n",
      "Successfully loaded cached run 7 with 121233 interactions\n",
      "✓ Successfully loaded run 7\n",
      "Loading run 8 (9/48)...\n",
      "Loading cached DataFrame for run 8 from results/kmarx_RRM1/csv_data/run_008.csv\n",
      "Successfully loaded cached run 8 with 114912 interactions\n",
      "✓ Successfully loaded run 8\n",
      "Loading run 9 (10/48)...\n",
      "Loading cached DataFrame for run 9 from results/kmarx_RRM1/csv_data/run_009.csv\n",
      "Successfully loaded cached run 9 with 118577 interactions\n",
      "✓ Successfully loaded run 9\n",
      "Loading run 10 (11/48)...\n",
      "Loading cached DataFrame for run 10 from results/kmarx_RRM1/csv_data/run_010.csv\n",
      "Successfully loaded cached run 10 with 115326 interactions\n",
      "✓ Successfully loaded run 10\n",
      "Loading run 11 (12/48)...\n",
      "Loading cached DataFrame for run 11 from results/kmarx_RRM1/csv_data/run_011.csv\n",
      "Successfully loaded cached run 11 with 114414 interactions\n",
      "✓ Successfully loaded run 11\n",
      "Loading run 12 (13/48)...\n",
      "Loading cached DataFrame for run 12 from results/kmarx_RRM1/csv_data/run_012.csv\n",
      "Successfully loaded cached run 12 with 114113 interactions\n",
      "✓ Successfully loaded run 12\n",
      "Loading run 13 (14/48)...\n",
      "Loading cached DataFrame for run 13 from results/kmarx_RRM1/csv_data/run_013.csv\n",
      "Successfully loaded cached run 13 with 113666 interactions\n",
      "✓ Successfully loaded run 13\n",
      "Loading run 14 (15/48)...\n",
      "Loading cached DataFrame for run 14 from results/kmarx_RRM1/csv_data/run_014.csv\n",
      "Successfully loaded cached run 14 with 106793 interactions\n",
      "✓ Successfully loaded run 14\n",
      "Loading run 15 (16/48)...\n",
      "Loading cached DataFrame for run 15 from results/kmarx_RRM1/csv_data/run_015.csv\n",
      "Successfully loaded cached run 15 with 115141 interactions\n",
      "✓ Successfully loaded run 15\n",
      "Loading run 16 (17/48)...\n",
      "Loading cached DataFrame for run 16 from results/kmarx_RRM1/csv_data/run_016.csv\n",
      "Successfully loaded cached run 16 with 116481 interactions\n",
      "✓ Successfully loaded run 16\n",
      "Loading run 17 (18/48)...\n",
      "Loading cached DataFrame for run 17 from results/kmarx_RRM1/csv_data/run_017.csv\n",
      "Successfully loaded cached run 17 with 123909 interactions\n",
      "✓ Successfully loaded run 17\n",
      "Loading run 18 (19/48)...\n",
      "Loading cached DataFrame for run 18 from results/kmarx_RRM1/csv_data/run_018.csv\n",
      "Successfully loaded cached run 18 with 114830 interactions\n",
      "✓ Successfully loaded run 18\n",
      "Loading run 19 (20/48)...\n",
      "Loading cached DataFrame for run 19 from results/kmarx_RRM1/csv_data/run_019.csv\n",
      "Successfully loaded cached run 19 with 132900 interactions\n",
      "✓ Successfully loaded run 19\n",
      "Loading run 20 (21/48)...\n",
      "Loading cached DataFrame for run 20 from results/kmarx_RRM1/csv_data/run_020.csv\n",
      "Successfully loaded cached run 20 with 119124 interactions\n",
      "✓ Successfully loaded run 20\n",
      "Loading run 21 (22/48)...\n",
      "Loading cached DataFrame for run 21 from results/kmarx_RRM1/csv_data/run_021.csv\n",
      "Successfully loaded cached run 21 with 115577 interactions\n",
      "✓ Successfully loaded run 21\n",
      "Loading run 22 (23/48)...\n",
      "Loading cached DataFrame for run 22 from results/kmarx_RRM1/csv_data/run_022.csv\n",
      "Successfully loaded cached run 22 with 114417 interactions\n",
      "✓ Successfully loaded run 22\n",
      "Loading run 23 (24/48)...\n",
      "Loading cached DataFrame for run 23 from results/kmarx_RRM1/csv_data/run_023.csv\n",
      "Successfully loaded cached run 23 with 113633 interactions\n",
      "✓ Successfully loaded run 23\n",
      "Loading run 24 (25/48)...\n",
      "Loading cached DataFrame for run 24 from results/kmarx_RRM1/csv_data/run_024.csv\n",
      "Successfully loaded cached run 24 with 122290 interactions\n",
      "✓ Successfully loaded run 24\n",
      "Loading run 25 (26/48)...\n",
      "Loading cached DataFrame for run 25 from results/kmarx_RRM1/csv_data/run_025.csv\n",
      "Successfully loaded cached run 25 with 115285 interactions\n",
      "✓ Successfully loaded run 25\n",
      "Loading run 26 (27/48)...\n",
      "Loading cached DataFrame for run 26 from results/kmarx_RRM1/csv_data/run_026.csv\n",
      "Successfully loaded cached run 26 with 122228 interactions\n",
      "✓ Successfully loaded run 26\n",
      "Loading run 27 (28/48)...\n",
      "Loading cached DataFrame for run 27 from results/kmarx_RRM1/csv_data/run_027.csv\n",
      "Successfully loaded cached run 27 with 112826 interactions\n",
      "✓ Successfully loaded run 27\n",
      "Loading run 28 (29/48)...\n",
      "Loading cached DataFrame for run 28 from results/kmarx_RRM1/csv_data/run_028.csv\n",
      "Successfully loaded cached run 28 with 127590 interactions\n",
      "✓ Successfully loaded run 28\n",
      "Loading run 29 (30/48)...\n",
      "Loading cached DataFrame for run 29 from results/kmarx_RRM1/csv_data/run_029.csv\n",
      "Successfully loaded cached run 29 with 119416 interactions\n",
      "✓ Successfully loaded run 29\n",
      "Loading run 30 (31/48)...\n",
      "Loading cached DataFrame for run 30 from results/kmarx_RRM1/csv_data/run_030.csv\n",
      "Successfully loaded cached run 30 with 99333 interactions\n",
      "✓ Successfully loaded run 30\n",
      "Loading run 31 (32/48)...\n",
      "Loading cached DataFrame for run 31 from results/kmarx_RRM1/csv_data/run_031.csv\n",
      "Successfully loaded cached run 31 with 115523 interactions\n",
      "✓ Successfully loaded run 31\n",
      "Loading run 32 (33/48)...\n",
      "Loading cached DataFrame for run 32 from results/kmarx_RRM1/csv_data/run_032.csv\n",
      "Successfully loaded cached run 32 with 163112 interactions\n",
      "✓ Successfully loaded run 32\n",
      "Loading run 33 (34/48)...\n",
      "Loading cached DataFrame for run 33 from results/kmarx_RRM1/csv_data/run_033.csv\n",
      "Successfully loaded cached run 33 with 120253 interactions\n",
      "✓ Successfully loaded run 33\n",
      "Loading run 34 (35/48)...\n",
      "Loading cached DataFrame for run 34 from results/kmarx_RRM1/csv_data/run_034.csv\n",
      "Successfully loaded cached run 34 with 144702 interactions\n",
      "✓ Successfully loaded run 34\n",
      "Loading run 35 (36/48)...\n",
      "Loading cached DataFrame for run 35 from results/kmarx_RRM1/csv_data/run_035.csv\n",
      "Successfully loaded cached run 35 with 120498 interactions\n",
      "✓ Successfully loaded run 35\n",
      "Loading run 36 (37/48)...\n",
      "Loading cached DataFrame for run 36 from results/kmarx_RRM1/csv_data/run_036.csv\n",
      "Successfully loaded cached run 36 with 117895 interactions\n",
      "✓ Successfully loaded run 36\n",
      "Loading run 37 (38/48)...\n",
      "Loading cached DataFrame for run 37 from results/kmarx_RRM1/csv_data/run_037.csv\n",
      "Successfully loaded cached run 37 with 162433 interactions\n",
      "✓ Successfully loaded run 37\n",
      "Loading run 38 (39/48)...\n",
      "Loading cached DataFrame for run 38 from results/kmarx_RRM1/csv_data/run_038.csv\n",
      "Successfully loaded cached run 38 with 162841 interactions\n",
      "✓ Successfully loaded run 38\n",
      "Loading run 39 (40/48)...\n",
      "Loading cached DataFrame for run 39 from results/kmarx_RRM1/csv_data/run_039.csv\n",
      "Successfully loaded cached run 39 with 106016 interactions\n",
      "✓ Successfully loaded run 39\n",
      "Loading run 40 (41/48)...\n",
      "Loading cached DataFrame for run 40 from results/kmarx_RRM1/csv_data/run_040.csv\n",
      "Successfully loaded cached run 40 with 96762 interactions\n",
      "✓ Successfully loaded run 40\n",
      "Loading run 41 (42/48)...\n",
      "Loading cached DataFrame for run 41 from results/kmarx_RRM1/csv_data/run_041.csv\n",
      "Successfully loaded cached run 41 with 146864 interactions\n",
      "✓ Successfully loaded run 41\n",
      "Loading run 42 (43/48)...\n",
      "Loading cached DataFrame for run 42 from results/kmarx_RRM1/csv_data/run_042.csv\n",
      "Successfully loaded cached run 42 with 96379 interactions\n",
      "✓ Successfully loaded run 42\n",
      "Loading run 43 (44/48)...\n",
      "Loading cached DataFrame for run 43 from results/kmarx_RRM1/csv_data/run_043.csv\n",
      "Successfully loaded cached run 43 with 119600 interactions\n",
      "✓ Successfully loaded run 43\n",
      "Loading run 44 (45/48)...\n",
      "Loading cached DataFrame for run 44 from results/kmarx_RRM1/csv_data/run_044.csv\n",
      "Successfully loaded cached run 44 with 133276 interactions\n",
      "✓ Successfully loaded run 44\n",
      "Loading run 45 (46/48)...\n",
      "Loading cached DataFrame for run 45 from results/kmarx_RRM1/csv_data/run_045.csv\n",
      "Successfully loaded cached run 45 with 125323 interactions\n",
      "✓ Successfully loaded run 45\n",
      "Loading run 46 (47/48)...\n",
      "Loading cached DataFrame for run 46 from results/kmarx_RRM1/csv_data/run_046.csv\n",
      "Successfully loaded cached run 46 with 132346 interactions\n",
      "✓ Successfully loaded run 46\n",
      "Loading run 47 (48/48)...\n",
      "Loading cached DataFrame for run 47 from results/kmarx_RRM1/csv_data/run_047.csv\n",
      "Successfully loaded cached run 47 with 124958 interactions\n",
      "✓ Successfully loaded run 47\n",
      "Successfully loaded 48 out of 48 runs\n",
      "Processing 25 contacts in frame 0...\n",
      "Identified 25 native contacts from run 0, frame 0.\n",
      "Total native contact energy at frame 0: 6.845\n",
      "🔍 Checking timeseries cache for 48 loaded runs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n",
      "📁 Loading cached timeseries for run 0\n",
      "✅ Loaded cached timeseries for run 0 (4010 frames)\n",
      "📁 Loading cached timeseries for run 1\n",
      "✅ Loaded cached timeseries for run 1 (3960 frames)\n",
      "📁 Loading cached timeseries for run 2\n",
      "✅ Loaded cached timeseries for run 2 (3990 frames)\n",
      "📁 Loading cached timeseries for run 3\n",
      "✅ Loaded cached timeseries for run 3 (4000 frames)\n",
      "📁 Loading cached timeseries for run 4\n",
      "✅ Loaded cached timeseries for run 4 (4050 frames)\n",
      "📁 Loading cached timeseries for run 5\n",
      "✅ Loaded cached timeseries for run 5 (4000 frames)\n",
      "📁 Loading cached timeseries for run 6\n",
      "✅ Loaded cached timeseries for run 6 (3980 frames)\n",
      "📁 Loading cached timeseries for run 7\n",
      "✅ Loaded cached timeseries for run 7 (3970 frames)\n",
      "📁 Loading cached timeseries for run 8\n",
      "✅ Loaded cached timeseries for run 8 (4000 frames)\n",
      "📁 Loading cached timeseries for run 9\n",
      "✅ Loaded cached timeseries for run 9 (3970 frames)\n",
      "📁 Loading cached timeseries for run 10\n",
      "✅ Loaded cached timeseries for run 10 (3920 frames)\n",
      "📁 Loading cached timeseries for run 11\n",
      "✅ Loaded cached timeseries for run 11 (4000 frames)\n",
      "📁 Loading cached timeseries for run 12\n",
      "✅ Loaded cached timeseries for run 12 (4010 frames)\n",
      "📁 Loading cached timeseries for run 13\n",
      "✅ Loaded cached timeseries for run 13 (3960 frames)\n",
      "📁 Loading cached timeseries for run 14\n",
      "✅ Loaded cached timeseries for run 14 (3970 frames)\n",
      "📁 Loading cached timeseries for run 15\n",
      "✅ Loaded cached timeseries for run 15 (4020 frames)\n",
      "📁 Loading cached timeseries for run 16\n",
      "✅ Loaded cached timeseries for run 16 (4000 frames)\n",
      "📁 Loading cached timeseries for run 17\n",
      "✅ Loaded cached timeseries for run 17 (3970 frames)\n",
      "📁 Loading cached timeseries for run 18\n",
      "✅ Loaded cached timeseries for run 18 (3970 frames)\n",
      "📁 Loading cached timeseries for run 19\n",
      "✅ Loaded cached timeseries for run 19 (4090 frames)\n",
      "📁 Loading cached timeseries for run 20\n",
      "✅ Loaded cached timeseries for run 20 (4060 frames)\n",
      "📁 Loading cached timeseries for run 21\n",
      "✅ Loaded cached timeseries for run 21 (3990 frames)\n",
      "📁 Loading cached timeseries for run 22\n",
      "✅ Loaded cached timeseries for run 22 (4030 frames)\n",
      "📁 Loading cached timeseries for run 23\n",
      "✅ Loaded cached timeseries for run 23 (4080 frames)\n",
      "📁 Loading cached timeseries for run 24\n",
      "✅ Loaded cached timeseries for run 24 (3980 frames)\n",
      "📁 Loading cached timeseries for run 25\n",
      "✅ Loaded cached timeseries for run 25 (4050 frames)\n",
      "📁 Loading cached timeseries for run 26\n",
      "✅ Loaded cached timeseries for run 26 (4000 frames)\n",
      "📁 Loading cached timeseries for run 27\n",
      "✅ Loaded cached timeseries for run 27 (4020 frames)\n",
      "📁 Loading cached timeseries for run 28\n",
      "✅ Loaded cached timeseries for run 28 (4100 frames)\n",
      "📁 Loading cached timeseries for run 29\n",
      "✅ Loaded cached timeseries for run 29 (4080 frames)\n",
      "📁 Loading cached timeseries for run 30\n",
      "✅ Loaded cached timeseries for run 30 (4090 frames)\n",
      "📁 Loading cached timeseries for run 31\n",
      "✅ Loaded cached timeseries for run 31 (4120 frames)\n",
      "📁 Loading cached timeseries for run 32\n",
      "✅ Loaded cached timeseries for run 32 (4250 frames)\n",
      "📁 Loading cached timeseries for run 33\n",
      "✅ Loaded cached timeseries for run 33 (4010 frames)\n",
      "📁 Loading cached timeseries for run 34\n",
      "✅ Loaded cached timeseries for run 34 (4140 frames)\n",
      "📁 Loading cached timeseries for run 35\n",
      "✅ Loaded cached timeseries for run 35 (4120 frames)\n",
      "📁 Loading cached timeseries for run 36\n",
      "✅ Loaded cached timeseries for run 36 (4210 frames)\n",
      "📁 Loading cached timeseries for run 37\n",
      "✅ Loaded cached timeseries for run 37 (4340 frames)\n",
      "📁 Loading cached timeseries for run 38\n",
      "✅ Loaded cached timeseries for run 38 (4410 frames)\n",
      "📁 Loading cached timeseries for run 39\n",
      "✅ Loaded cached timeseries for run 39 (4120 frames)\n",
      "📁 Loading cached timeseries for run 40\n",
      "✅ Loaded cached timeseries for run 40 (4330 frames)\n",
      "📁 Loading cached timeseries for run 41\n",
      "✅ Loaded cached timeseries for run 41 (4570 frames)\n",
      "📁 Loading cached timeseries for run 42\n",
      "✅ Loaded cached timeseries for run 42 (4250 frames)\n",
      "📁 Loading cached timeseries for run 43\n",
      "✅ Loaded cached timeseries for run 43 (4600 frames)\n",
      "📁 Loading cached timeseries for run 44\n",
      "✅ Loaded cached timeseries for run 44 (5990 frames)\n",
      "📁 Loading cached timeseries for run 45\n",
      "✅ Loaded cached timeseries for run 45 (5860 frames)\n",
      "📁 Loading cached timeseries for run 46\n",
      "✅ Loaded cached timeseries for run 46 (6470 frames)\n",
      "📁 Loading cached timeseries for run 47\n",
      "✅ Loaded cached timeseries for run 47 (6740 frames)\n",
      "📊 Cache summary:\n",
      "   ✅ Found cached: 48 runs\n",
      "   🔄 Need to calculate: 0 runs []\n",
      "\n",
      "🔗 Combining results from 48 runs...\n",
      "✅ Combined timeseries contains 48 runs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n",
      "💾 Saved updated combined cache to results/kmarx_RRM1/csv_data/all_runs_native_timeseries.csv\n",
      "Saving individual run plots to results/kmarx_RRM1/plots...\n",
      "Found 48 unique runs in timeseries data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n",
      "Found 48 existing plot files\n",
      "Creating plot 1/48: Run 0\n",
      "  📊 Run 0 has 4010 data points (frames 0-4009)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_000_native_contacts.png\n",
      "Creating plot 2/48: Run 1\n",
      "  📊 Run 1 has 3960 data points (frames 0-3959)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_001_native_contacts.png\n",
      "Creating plot 3/48: Run 2\n",
      "  📊 Run 2 has 3990 data points (frames 0-3989)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_002_native_contacts.png\n",
      "Creating plot 4/48: Run 3\n",
      "  📊 Run 3 has 4000 data points (frames 0-3999)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_003_native_contacts.png\n",
      "Creating plot 5/48: Run 4\n",
      "  📊 Run 4 has 4050 data points (frames 0-4049)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_004_native_contacts.png\n",
      "Creating plot 6/48: Run 5\n",
      "  📊 Run 5 has 4000 data points (frames 0-3999)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_005_native_contacts.png\n",
      "Creating plot 7/48: Run 6\n",
      "  📊 Run 6 has 3980 data points (frames 0-3979)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_006_native_contacts.png\n",
      "Creating plot 8/48: Run 7\n",
      "  📊 Run 7 has 3970 data points (frames 0-3969)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_007_native_contacts.png\n",
      "Creating plot 9/48: Run 8\n",
      "  📊 Run 8 has 4000 data points (frames 0-3999)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_008_native_contacts.png\n",
      "Creating plot 10/48: Run 9\n",
      "  📊 Run 9 has 3970 data points (frames 0-3969)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_009_native_contacts.png\n",
      "Creating plot 11/48: Run 10\n",
      "  📊 Run 10 has 3920 data points (frames 0-3919)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_010_native_contacts.png\n",
      "Creating plot 12/48: Run 11\n",
      "  📊 Run 11 has 4000 data points (frames 0-3999)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_011_native_contacts.png\n",
      "Creating plot 13/48: Run 12\n",
      "  📊 Run 12 has 4010 data points (frames 0-4009)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_012_native_contacts.png\n",
      "Creating plot 14/48: Run 13\n",
      "  📊 Run 13 has 3960 data points (frames 0-3959)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_013_native_contacts.png\n",
      "Creating plot 15/48: Run 14\n",
      "  📊 Run 14 has 3970 data points (frames 0-3969)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_014_native_contacts.png\n",
      "Creating plot 16/48: Run 15\n",
      "  📊 Run 15 has 4020 data points (frames 0-4019)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_015_native_contacts.png\n",
      "Creating plot 17/48: Run 16\n",
      "  📊 Run 16 has 4000 data points (frames 0-3999)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_016_native_contacts.png\n",
      "Creating plot 18/48: Run 17\n",
      "  📊 Run 17 has 3970 data points (frames 0-3969)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_017_native_contacts.png\n",
      "Creating plot 19/48: Run 18\n",
      "  📊 Run 18 has 3970 data points (frames 0-3969)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_018_native_contacts.png\n",
      "Creating plot 20/48: Run 19\n",
      "  📊 Run 19 has 4090 data points (frames 0-4089)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_019_native_contacts.png\n",
      "Creating plot 21/48: Run 20\n",
      "  📊 Run 20 has 4060 data points (frames 0-4059)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_020_native_contacts.png\n",
      "Creating plot 22/48: Run 21\n",
      "  📊 Run 21 has 3990 data points (frames 0-3989)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_021_native_contacts.png\n",
      "Creating plot 23/48: Run 22\n",
      "  📊 Run 22 has 4030 data points (frames 0-4029)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_022_native_contacts.png\n",
      "Creating plot 24/48: Run 23\n",
      "  📊 Run 23 has 4080 data points (frames 0-4079)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_023_native_contacts.png\n",
      "Creating plot 25/48: Run 24\n",
      "  📊 Run 24 has 3980 data points (frames 0-3979)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_024_native_contacts.png\n",
      "Creating plot 26/48: Run 25\n",
      "  📊 Run 25 has 4050 data points (frames 0-4049)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_025_native_contacts.png\n",
      "Creating plot 27/48: Run 26\n",
      "  📊 Run 26 has 4000 data points (frames 0-3999)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_026_native_contacts.png\n",
      "Creating plot 28/48: Run 27\n",
      "  📊 Run 27 has 4020 data points (frames 0-4019)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_027_native_contacts.png\n",
      "Creating plot 29/48: Run 28\n",
      "  📊 Run 28 has 4100 data points (frames 0-4099)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_028_native_contacts.png\n",
      "Creating plot 30/48: Run 29\n",
      "  📊 Run 29 has 4080 data points (frames 0-4079)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_029_native_contacts.png\n",
      "Creating plot 31/48: Run 30\n",
      "  📊 Run 30 has 4090 data points (frames 0-4089)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_030_native_contacts.png\n",
      "Creating plot 32/48: Run 31\n",
      "  📊 Run 31 has 4120 data points (frames 0-4119)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_031_native_contacts.png\n",
      "Creating plot 33/48: Run 32\n",
      "  📊 Run 32 has 4250 data points (frames 0-4249)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_032_native_contacts.png\n",
      "Creating plot 34/48: Run 33\n",
      "  📊 Run 33 has 4010 data points (frames 0-4009)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_033_native_contacts.png\n",
      "Creating plot 35/48: Run 34\n",
      "  📊 Run 34 has 4140 data points (frames 0-4139)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_034_native_contacts.png\n",
      "Creating plot 36/48: Run 35\n",
      "  📊 Run 35 has 4120 data points (frames 0-4119)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_035_native_contacts.png\n",
      "Creating plot 37/48: Run 36\n",
      "  📊 Run 36 has 4210 data points (frames 0-4209)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_036_native_contacts.png\n",
      "Creating plot 38/48: Run 37\n",
      "  📊 Run 37 has 4340 data points (frames 0-4339)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_037_native_contacts.png\n",
      "Creating plot 39/48: Run 38\n",
      "  📊 Run 38 has 4410 data points (frames 0-4409)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_038_native_contacts.png\n",
      "Creating plot 40/48: Run 39\n",
      "  📊 Run 39 has 4120 data points (frames 0-4119)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_039_native_contacts.png\n",
      "Creating plot 41/48: Run 40\n",
      "  📊 Run 40 has 4330 data points (frames 0-4329)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_040_native_contacts.png\n",
      "Creating plot 42/48: Run 41\n",
      "  📊 Run 41 has 4570 data points (frames 0-4569)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_041_native_contacts.png\n",
      "Creating plot 43/48: Run 42\n",
      "  📊 Run 42 has 4250 data points (frames 0-4249)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_042_native_contacts.png\n",
      "Creating plot 44/48: Run 43\n",
      "  📊 Run 43 has 4600 data points (frames 0-4599)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_043_native_contacts.png\n",
      "Creating plot 45/48: Run 44\n",
      "  📊 Run 44 has 5990 data points (frames 0-5989)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_044_native_contacts.png\n",
      "Creating plot 46/48: Run 45\n",
      "  📊 Run 45 has 5860 data points (frames 0-5859)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_045_native_contacts.png\n",
      "Creating plot 47/48: Run 46\n",
      "  📊 Run 46 has 6470 data points (frames 0-6469)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_046_native_contacts.png\n",
      "Creating plot 48/48: Run 47\n",
      "  📊 Run 47 has 6740 data points (frames 0-6739)\n",
      "  ✅ Saved: results/kmarx_RRM1/plots/run_047_native_contacts.png\n",
      "\n",
      "🎯 FINAL SUMMARY:\n",
      "  ✅ Successfully created 48 individual plot files\n",
      "  ❌ Failed to create 0 plots\n",
      "  📁 Total PNG files in directory: 48\n",
      "  📁 PNG files: ['run_000_native_contacts.png', 'run_001_native_contacts.png', 'run_002_native_contacts.png', 'run_003_native_contacts.png', 'run_004_native_contacts.png', 'run_005_native_contacts.png', 'run_006_native_contacts.png', 'run_007_native_contacts.png', 'run_008_native_contacts.png', 'run_009_native_contacts.png', 'run_010_native_contacts.png', 'run_011_native_contacts.png', 'run_012_native_contacts.png', 'run_013_native_contacts.png', 'run_014_native_contacts.png', 'run_015_native_contacts.png', 'run_016_native_contacts.png', 'run_017_native_contacts.png', 'run_018_native_contacts.png', 'run_019_native_contacts.png', 'run_020_native_contacts.png', 'run_021_native_contacts.png', 'run_022_native_contacts.png', 'run_023_native_contacts.png', 'run_024_native_contacts.png', 'run_025_native_contacts.png', 'run_026_native_contacts.png', 'run_027_native_contacts.png', 'run_028_native_contacts.png', 'run_029_native_contacts.png', 'run_030_native_contacts.png', 'run_031_native_contacts.png', 'run_032_native_contacts.png', 'run_033_native_contacts.png', 'run_034_native_contacts.png', 'run_035_native_contacts.png', 'run_036_native_contacts.png', 'run_037_native_contacts.png', 'run_038_native_contacts.png', 'run_039_native_contacts.png', 'run_040_native_contacts.png', 'run_041_native_contacts.png', 'run_042_native_contacts.png', 'run_043_native_contacts.png', 'run_044_native_contacts.png', 'run_045_native_contacts.png', 'run_046_native_contacts.png', 'run_047_native_contacts.png']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create analyzer instance\n",
    "analyzer = NativeContactsAnalyzer(results_dir=\"results/kmarx_RRM1/\", csv_dir=\"results/kmarx_RRM1/csv_data\",file_pattern=\"Kmarx_Pab1_RRM1\")\n",
    "\n",
    "# Load all available runs (this will save CSVs automatically)\n",
    "analyzer.load_all_runs(max_runs=48)  # Limit to first 5 runs for testing\n",
    "\n",
    "# Identify native contacts from run 0, frame 0\n",
    "analyzer.identify_native_contacts(run_number=0)\n",
    "\n",
    "# Calculate time series for all runs\n",
    "timeseries_data = analyzer.calculate_all_native_contacts_timeseries()\n",
    "\n",
    "# Plot the results\n",
    "\n",
    "analyzer.plot_and_save_individual_runs(timeseries_data,plot_dir=\"results/kmarx_RRM1/plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Load individual runs\n",
    "# analyzer = NativeContactsAnalyzer()\n",
    "# analyzer.load_single_run(0, show_timing=True)\n",
    "# analyzer.load_single_run(1, show_timing=True)\n",
    "# analyzer.load_single_run(2, show_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from saved CSV files\n",
    "# analyzer = NativeContactsAnalyzer()\n",
    "# analyzer.load_run_from_csv(0)\n",
    "# analyzer.load_run_from_csv(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "# summary = analyzer.get_summary_statistics(timeseries_data)\n",
    "# print(\"Summary Statistics:\")\n",
    "# for key, value in summary.items():\n",
    "#     print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
